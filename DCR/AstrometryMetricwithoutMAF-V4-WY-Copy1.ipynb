{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comments beginning with #BEE were written by bee martin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import astropy\n",
    "from astropy.io import ascii\n",
    "import numpy as np\n",
    "import emcee\n",
    "from scipy.optimize import minimize\n",
    "from numpy.random import normal\n",
    "from numpy.random import uniform\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import palettable\n",
    "import richardsplot as rplot\n",
    "%matplotlib inline\n",
    "import random\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    "rc('text', usetex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open file with photo-z PDF redshift bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: read in table of redshifts and save the 'zshifts' column as a variable named zshifts\n",
    "#BEE: zshifts is a list of redshifts from 0.4 to 4.0\n",
    "#GTR: This is just a list of redshift bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zshifts_Table = ascii.read('fittingS82_zshifts.dat', format='csv')\n",
    "zshifts = zshifts_Table['zshifts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open file with regression values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: create an array of sdss features\n",
    "#BEE: read in table of regression values, create array of zeros with shape(features, redshifts)\n",
    "#BEE: fill array of zeros with data from regression values table\n",
    "#GTR: These are the mean colors and DCR slopes for the above redshift bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_features = ['u-g', 'g-r', 'r-i', 'i-z']\n",
    "sdss_features_dcr = ['u-g', 'g-r', 'r-i', 'i-z', 'u-slope', 'g-slope']\n",
    "\n",
    "color_fit_Table = ascii.read('fittingS82_zshiftfit.dat')\n",
    "color_fit_Table.remove_column('col1')\n",
    "color_fit = np.zeros((len(sdss_features), len(zshifts)))\n",
    "color_fit_dcr = np.zeros((len(sdss_features_dcr), len(zshifts)))\n",
    "for i in range(len(sdss_features)):\n",
    "    for j in range(len(zshifts)):\n",
    "        color_fit[i,j] = np.asarray(color_fit_Table[i][j])\n",
    "\n",
    "for i in range(len(sdss_features_dcr)):\n",
    "    for j in range(len(zshifts)):\n",
    "        color_fit_dcr[i,j] = np.asarray(color_fit_Table[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open file with regression covariance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: read in regression covariance data\n",
    "#BEE: create array of zeros with shape (features, features, redshifts), fill it with covariance table data\n",
    "#GTR: These are the covariances between each of the above parameters at each redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_covariance_Table = ascii.read('fittingS82_zshiftcovariance.dat')\n",
    "color_covariance_Table.remove_column('col1')\n",
    "color_covariance_Table.remove_column('col2')\n",
    "color_covariance = np.zeros((len(sdss_features), len(sdss_features), len(zshifts)))\n",
    "color_covariance_dcr = np.zeros((len(sdss_features_dcr), len(sdss_features_dcr), len(zshifts)))  \n",
    "l = 0\n",
    "for i in range(len(sdss_features_dcr)):\n",
    "    for j in range(len(sdss_features_dcr)):\n",
    "        for k in range(len(zshifts)):\n",
    "            color_covariance_dcr[i,j,k] = np.asarray(color_covariance_Table[l][k])\n",
    "        l += 1\n",
    "color_covariance = color_covariance_dcr[:4, :4, :]\n",
    "#print(color_covariance_dcr)\n",
    "#print(color_covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open file with the simulated quasar true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: Read in simulated \"true\" quasar data\n",
    "#GTR: These are simulated quasars with simulated parameters (and their errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zspec', 'u-g', 'g-r', 'r-i', 'i-z', 'u-slope', 'g-slope', 'uerr', 'gerr', 'rerr', 'ierr', 'zerr', 'u-slopeerr', 'g-slopeerr']\n"
     ]
    }
   ],
   "source": [
    "test_quasars0 = ascii.read('random_quasars.dat')\n",
    "test_quasars = ascii.read('random_quasars100k.dat')[:1000]\n",
    "print(test_quasars.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: simulate airmass observations in u ang g\n",
    "#GTR: We ignore the next cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "astrometric_error = [0.035,0.025]  #[u-band error, g-band error]\n",
    "\n",
    "airmasses = uniform(low=1.0, high=1.3, size=50)\n",
    "airmasses = np.append(airmasses, uniform(low=1.3, high=2.0, size=14))\n",
    "\n",
    "filters = np.tile(['u', 'g'], int(len(airmasses)/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: this cell will take observations from the OpSim rather than simulating them\n",
    "#GTR: Not sure exactly where this opSim information comes from.  Weixiang?\n",
    "#id.csv is just an indexed list of RA and Dec\n",
    "#dcr_all.csv is a list of observation parameters for each of those IDs\n",
    "#this includes airmass and filter, which is all that we use right now?\n",
    "#It seems that right now a random object is being chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.035, 0.025]\n"
     ]
    }
   ],
   "source": [
    "astrometric_error = [0.035, 0.025]\n",
    "#astrometric_error = np.multiply(astrometric_error, [2,2])\n",
    "print(astrometric_error)\n",
    "# Weixiang: import opsim cadence after fix for python2\n",
    "ids = pd.read_csv('id.csv')\n",
    "cad = pd.read_csv('dcr_all.csv')\n",
    "\n",
    "#pick random object's cadence\n",
    "random_cadence = random.randint(0,max(cad['id']))\n",
    "# assign the cadence of random object to dcr_0\n",
    "dcr_0 = cad[cad['id'] == random_cadence].copy()\n",
    "obs_g = dcr_0[dcr_0['filter'] == 'g']\n",
    "obs_u = dcr_0[dcr_0['filter'] == 'u']\n",
    "obs = np.concatenate((obs_g, obs_u))\n",
    "\n",
    "### Orginal code to import cadence\n",
    "# dcr = np.load('dcr.npz')\n",
    "# print(list(dcr.keys()))\n",
    "# dcrra_dec = dcr['ra_dec']\n",
    "# dcrdata = dcr['data']\n",
    "# print(dcrra_dec[0])\n",
    "# obs_g = dcrdata[0][dcrdata[0]['filter']=='g']\n",
    "# obs_u = dcrdata[0][dcrdata[0]['filter']=='u']\n",
    "# obs = np.concatenate((obs_g, obs_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTR: (24 July 2020) I don't recall what these comments are about.  Should take another look at them.\n",
    "\n",
    "GTR: Split out cell that defines airmasses.  Just define one at a time.  Predefine the experiments and comment out the ones being run each time.  Make sure that the output files are unique for each experiment.\n",
    "\n",
    "GTR: Run colors only and colors+normal DCR just once.  We don't need to run those again.  But those can be the first 2 \"experiments\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: Extract the airmass and filters for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weixiang: modified the item index to match the order of columns in new file\n",
    "airmasses = np.array([item[3] for item in obs])\n",
    "filters = np.array([item[5] for item in obs])\n",
    "\n",
    "#airmasses_long = np.append(airmasses, [1.6, 1.6])\n",
    "#filters_long = np.append(filters, ['g', 'g'])\n",
    "#airmasses_twilight = np.append(airmasses, [2.0, 2.0])\n",
    "#filters_twilight = np.append(filters, ['g', 'g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEE: The next cell is a switch that lets you choose the experiment to run.  There are 2 types of experiments: 'substitution' and 'addition'.  Change the string in the cell to either 'substitution' or 'addition'.  The airmasses should be 1.6, 1.7, 1.8, 1.9, or 2.0.  In the case of addition, you can set airmass_to_use to an array of airmasses and it will add all of them.  NOTE: Make sure, if you're running multiple experiments, to run the cell above for each one so you don't overwrite the wrong airmasses array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: Let's not do that experiment any more and just explore the different opSims.  \n",
    "#So either take this out or just leave the array blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_to_run = 'addition'\n",
    "#experiment_to_run = 'substitution'\n",
    "#experiment_to_run = 'addition'\n",
    "airmass_to_use = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_to_run == 'colors':\n",
    "    save_file_name = 'AstroMetric_Colors_noDCR.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_to_run == 'substitution':    \n",
    "    airmass_to_substitute = airmass_to_use[0]\n",
    "    index_of_lowest = np.argmin(airmasses)\n",
    "    airmasses[index_of_lowest] = airmass_to_substitute\n",
    "    save_file_name = 'AstroMetric_SubstitutionDCR_' + str(int(airmass_to_substitute*10)) + '.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_to_run == 'addition':\n",
    "    filters_to_add = np.tile('g', int(len(airmass_to_use)))\n",
    "    airmasses = np.append(airmasses, airmass_to_use)\n",
    "    filters = np.append(filters, filters_to_add)\n",
    "    save_file_name = 'AstroMetric_TwilightDCR_' + str([int(airmass_to_use[i]*10) for i in range(len(airmass_to_use))]) + '.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: Not sure why this is here\n",
    "#and not clear that this file name is being used\n",
    "#I think that Bee was just trying to compare the results after 20 and 3 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "#airmass removal cell\n",
    "print(len(airmasses))\n",
    "#if you don't want to remove any, set number_to_leave to \"all\"\n",
    "number_to_leave = 20\n",
    "number_to_leave=\"all\"\n",
    "if number_to_leave != \"all\":\n",
    "    save_file_name = save_file_name[:-4] + \"_\" + str(number_to_leave) + \"obs\" + save_file_name[-4:]\n",
    "    print(\"file name is \" + save_file_name)\n",
    "    number_to_remove = len(airmasses) - number_to_leave\n",
    "else:\n",
    "    number_to_remove = 0\n",
    "removed = 0\n",
    "while removed < number_to_remove:\n",
    "    remove_index = random.randint(0,len(airmasses)-1)\n",
    "    airmasses = np.delete(airmasses, remove_index)\n",
    "    filters = np.delete(filters, remove_index)\n",
    "    removed += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "[1.00622878 1.00853972 1.01967367 1.01444427 1.00743023 1.03187831\n",
      " 1.02394573 1.00923144 1.0313592  1.00504214 1.18335984 1.02369731\n",
      " 1.07047894 1.01751372 1.0176494  1.00222458 1.00719521 1.00393647\n",
      " 1.00904816 1.03125394 1.07130849 1.01943226 1.01112162 1.01790893\n",
      " 1.0139087  1.00696787 1.04467149 1.00689994 1.00877272 1.02842539\n",
      " 1.01824907 1.01117482 1.02434356 1.013733   1.01019995 1.00620763\n",
      " 1.04033332 1.00259324 1.08969973 1.02348594 1.00390752 1.04807564\n",
      " 1.01139731 1.06130187 1.00223335 1.03758085 1.00631991 1.00661383\n",
      " 1.01090133 1.01637917 1.01998853 1.02433815 1.0250975  1.0187453\n",
      " 1.0445431  1.00717106 1.01155884 1.01358896 1.00267592 1.06361743\n",
      " 1.01787706 1.01890238 1.02765961 1.02014511 1.05615775 1.13835903\n",
      " 1.02252358 1.01385617 1.00496107 1.04433465 1.05973264 1.03202643\n",
      " 1.02071543 1.0080826  1.0395498  1.31922158 1.07229784 1.01912032\n",
      " 1.0591391  1.00499052 1.01146673 1.00523885 1.01122037 1.00265338\n",
      " 1.0049515  1.04461183 1.0024018  1.00380577 1.00555118 1.00682887\n",
      " 1.10474625 1.00837699 1.00590522 1.00606002 1.00618255 1.00424335\n",
      " 1.00634672 1.00280148 1.00322669 1.00521539 1.0130969  1.0026564\n",
      " 1.00579692 1.00466497 1.00948438 1.00258582 1.00505005 1.00254061\n",
      " 1.00943895 1.02029972 1.00319421 1.00315131 1.01430937 1.0043783\n",
      " 1.00924025 1.00365775 1.00418824 1.00576752 1.00544446 1.00587349\n",
      " 1.00457734 1.00394526 1.00935331 1.01366181 1.00283278 1.00616784\n",
      " 1.00257938 1.00270615 1.00486104 1.00284069 1.00714597 1.00511903\n",
      " 1.00906101 1.00285442 1.00238284 1.00866088 1.00297759 1.00597133]\n",
      "['g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g'\n",
      " 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'g' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      " 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      " 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u'\n",
      " 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u' 'u']\n",
      "AstroMetric_TwilightDCR_[].npz\n"
     ]
    }
   ],
   "source": [
    "print(len(airmasses))\n",
    "print(airmasses)\n",
    "print(filters)\n",
    "print(save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: I think that this is just to provide a basis of comparison with just a few (here 3) epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file name is AstroMetric_TwilightDCR_[]_3obs.npz\n"
     ]
    }
   ],
   "source": [
    "airmasses_20 = airmasses\n",
    "filters_20 = filters\n",
    "if experiment_to_run == 'addition':\n",
    "    filters_to_add = np.tile('g', int(len(airmass_to_use)))\n",
    "    airmasses = np.append(airmasses, airmass_to_use)\n",
    "    filters = np.append(filters, filters_to_add)\n",
    "    save_file_name = 'AstroMetric_TwilightDCR_' + str([int(airmass_to_use[i]*10) for i in range(len(airmass_to_use))]) + '.npz'\n",
    "number_to_leave = 3\n",
    "if number_to_leave != \"all\":\n",
    "    save_file_name = save_file_name[:-4] + \"_\" + str(number_to_leave) + \"obs\" + save_file_name[-4:]\n",
    "    print(\"file name is \" + save_file_name)\n",
    "    number_to_remove = len(airmasses) - number_to_leave\n",
    "else:\n",
    "    number_to_remove = 0\n",
    "removed = 0\n",
    "while removed < number_to_remove:\n",
    "    remove_index = random.randint(0,len(airmasses)-1)\n",
    "    airmasses = np.delete(airmasses, remove_index)\n",
    "    filters = np.delete(filters, remove_index)\n",
    "    removed += 1 \n",
    "airmasses_3 = airmasses\n",
    "filters_3 = filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate observed slopes from true slopes and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEE: lnlike calculates the loglikelihood, lnprior creates a prior on our linear fits, lnprob adds the prior to lnlike\n",
    "#BEE: run_fit runs the mcmc walkers over a range of linear fits and selects the median as the best fit and half the \n",
    "#     difference between 16th and 84th percentiles as the error\n",
    "#GTR: run_fit is computing the slope in the offset vs. tanZ plane for a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnlike(theta, x, y, yerr):\n",
    "    m, lnf = theta\n",
    "    model = m*x\n",
    "    inv_sigma2 = 1.0/(yerr**2. + model**2.*np.exp(2.*lnf))\n",
    "    return -0.5*(np.sum(((y-model)**2.*inv_sigma2 - np.log(inv_sigma2))))\n",
    "\n",
    "def lnprior(theta):\n",
    "    m, lnf = theta\n",
    "    if (-1.0 < m < 1.0) and (-100.0 < lnf < 100.0):\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def lnprob(theta, x, y, yerr):\n",
    "    lp = lnprior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + lnlike(theta, x, y, yerr)\n",
    "\n",
    "def run_fit(tanZList, RList, RerrList):\n",
    "    nll = lambda *args: -lnprob(*args)\n",
    "    x = np.copy(tanZList)\n",
    "    y = np.copy(RList)\n",
    "    yerr = np.copy(RerrList)\n",
    "    #first do a simple minimization to get starting values for mcmc\n",
    "    pm = np.random.choice([-1.0,1.0], size=len(x), replace=True)\n",
    "    result = minimize(nll, [-0.001, np.log(0.5)], args=(x, y, yerr))\n",
    "    m_ml, lnf_ml = result[\"x\"]\n",
    "    #now run mcmc\n",
    "    ndim, nwalkers = 2, 100\n",
    "    pos = [result[\"x\"] + 1e-4*np.random.randn(ndim) for i in range(nwalkers)]\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=(x, y, yerr))\n",
    "    sampler.run_mcmc(pos, 500)\n",
    "    samples = sampler.chain[:, 50:, :].reshape((-1, ndim))\n",
    "    ms = samples[np.random.randint(len(samples), size=100)][:,0]\n",
    "    # return the median walker as the best slope and the half the 16-84th percentiles as the error\n",
    "    m_mcmc, lnf_mcmc = map(lambda v: (v[1]), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "    merr_mcmc, lnf_mcmc = map(lambda v: (0.5*(v[2]-v[0])), zip(*np.percentile(samples, [16, 50, 84], axis=0)))\n",
    "    return m_mcmc, merr_mcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTR: Split out cells that define functions from cells that make calls to those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: dcrSlopeCalc is computing the slope in the offset vs. tanZ plane for all the objects, calling run_fit for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcrSlopeCalc(airmasses, filters, test_quasars, makePlot = True):\n",
    "    astrometric_error = [0.035, 0.025]\n",
    "    obs_slopes_u    = np.zeros((len(test_quasars)))\n",
    "    obs_slopes_uerr = np.zeros((len(test_quasars)))\n",
    "    obs_slopes_g    = np.zeros((len(test_quasars)))\n",
    "    obs_slopes_gerr = np.zeros((len(test_quasars)))\n",
    "    imgNumString = 0\n",
    "    xAxis = np.linspace(0, 2.0, 100)\n",
    "    for i in range(len(test_quasars)):\n",
    "        true_slope_u = test_quasars['u-slope'][i]\n",
    "        true_slope_g = test_quasars['g-slope'][i]\n",
    "    \n",
    "        tanZList_u = np.array([])\n",
    "        RerrList_u = np.array([])\n",
    "        RList_u = np.array([])\n",
    "        tanZList_g = np.array([])\n",
    "        RerrList_g = np.array([])\n",
    "        RList_g = np.array([])\n",
    "    \n",
    "        for j, airmass in enumerate(airmasses):\n",
    "            tanZ_obs = np.tan(np.arccos(1.0/airmass)) #tangent of zenith angle of this observation\n",
    "            if filters[j] == 'u':\n",
    "                #calculate the observed offset\n",
    "                #random scatter around the true offset using a normal distribution with the astrometric error as the standard deviation\n",
    "                R_obs = normal(true_slope_u*tanZ_obs, astrometric_error[0])\n",
    "                tanZList_u = np.append(tanZList_u, tanZ_obs)              #list of x axis values\n",
    "                RerrList_u = np.append(RerrList_u, astrometric_error[0])  #list of y axis error values\n",
    "                RList_u = np.append(RList_u, R_obs)                       #list of y axis values\n",
    "            if filters[j] == 'g':\n",
    "                R_obs = normal(true_slope_g*tanZ_obs, astrometric_error[1])\n",
    "                tanZList_g = np.append(tanZList_g, tanZ_obs)\n",
    "                RerrList_g = np.append(RerrList_g, astrometric_error[1])\n",
    "                RList_g = np.append(RList_g, R_obs)\n",
    "    \n",
    "        # fit a stright line through the x and y values, using the y-err values\n",
    "        m_mcmc_u, merr_mcmc_u = run_fit(tanZList_u, RList_u, RerrList_u)\n",
    "        m_mcmc_g, merr_mcmc_g = run_fit(tanZList_g, RList_g, RerrList_g)\n",
    "        if makePlot == True:\n",
    "            bestFitLine_u = m_mcmc_u*xAxis + 0.0\n",
    "            bestFitLine_g = m_mcmc_g*xAxis + 0.0\n",
    "            trueFitLine_u = true_slope_u*xAxis + 0.0\n",
    "            trueFitLine_g = true_slope_g*xAxis + 0.0\n",
    "            plt.figure(figsize=(12,12))\n",
    "            plt.subplot(121)\n",
    "            plt.title('u-band observations + fit')\n",
    "            plt.scatter(tanZList_u, RList_u, label = 'Observations')\n",
    "            plt.plot(xAxis, bestFitLine_u, label='Fit Line')\n",
    "            plt.plot(xAxis, trueFitLine_u, label = 'True Line')\n",
    "            plt.legend()\n",
    "            plt.xlabel('Tan(Z)')\n",
    "            plt.ylabel('delta R')\n",
    "            plt.xlim(0.0, 2.0)\n",
    "            plt.scatter(x=tanZList_u, y=RList_u)\n",
    "            plt.subplot(122)\n",
    "            plt.title('g-band observations + fit')\n",
    "            plt.scatter(tanZList_g, RList_g, label = 'Observations')\n",
    "            plt.plot(xAxis, bestFitLine_g, label = 'Fit Line')\n",
    "            plt.plot(xAxis, trueFitLine_g, label = 'True Line')\n",
    "            plt.xlabel('Tan(Z)')\n",
    "            plt.xlim(0.0, 2.0)\n",
    "            plt.scatter(x=tanZList_g, y=RList_g)\n",
    "            filename = \"TanZimgFiles/airmassOffsetFit\"+str(len(airmasses))+\"_\"+\"{:0>5d}\".format(imgNumString)\n",
    "            plt.savefig(filename)\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "            imgNumString += 1\n",
    "        obs_slopes_u[i] = m_mcmc_u\n",
    "        obs_slopes_uerr[i] = merr_mcmc_u\n",
    "        obs_slopes_g[i] = m_mcmc_g\n",
    "        obs_slopes_gerr[i] = merr_mcmc_g\n",
    "    if makePlot == True:\n",
    "        deltaSlope_u = []\n",
    "        deltaSlope_g = []\n",
    "        for i in range(len(obs_slopes_u)):\n",
    "            deltaSlope_u = np.append(deltaSlope_u, test_quasars['u-slope'][i] - obs_slopes_u[i])\n",
    "        for i in range(len(obs_slopes_g)):\n",
    "            deltaSlope_g = np.append(deltaSlope_g, test_quasars['g-slope'][i] - obs_slopes_g[i])\n",
    "        plt.figure(figsize=(12,12))\n",
    "        plt.subplot(121)\n",
    "        plt.hist(deltaSlope_u, bins=50, range=(-0.3,0.3))\n",
    "        plt.title('Delta Slope u-band '+str(len(airmasses)))\n",
    "        plt.subplot(122)\n",
    "        plt.hist(deltaSlope_g, bins=50, range=(-0.3,0.3))\n",
    "        plt.title('Delta Slope g-band '+str(len(airmasses)))\n",
    "        filename = \"DeltaSlopeimgFiles/deltaSlopeHist\" + str(len(airmasses))\n",
    "        plt.savefig(filename)\n",
    "    return obs_slopes_u, obs_slopes_uerr, obs_slopes_g, obs_slopes_gerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: This cell actually calls the code that computes the slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['serif'] not found. Falling back to DejaVu Sans.\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    }
   ],
   "source": [
    "obs_slopes_u_20, obs_slopes_uerr, obs_slopes_g_20, obs_slopes_gerr = dcrSlopeCalc(airmasses_20, filters_20, test_quasars)\n",
    "obs_slopes_u_3, obs_slopes_uerr, obs_slopes_g_3, obs_slopes_gerr = dcrSlopeCalc(airmasses_3, filters_3, test_quasars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_indices = np.argsort(test_quasars['zspec'])\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(211)\n",
    "plt.title('Observed DCR Slopes vs. Redshift')\n",
    "plt.scatter(test_quasars['zspec'][sort_indices], test_quasars['u-slope'][sort_indices], color='red', label = 'True u slope')\n",
    "plt.plot(test_quasars['zspec'][sort_indices], obs_slopes_u_20[sort_indices], color='black', label = 'Observed u slope@20 obs', alpha=0.7)\n",
    "plt.plot(test_quasars['zspec'][sort_indices], obs_slopes_u_3[sort_indices], color='magenta',alpha=0.5, label = 'Observed u slope@3 obs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('u-band DCR slope')\n",
    "plt.subplot(212)\n",
    "plt.scatter(test_quasars['zspec'][sort_indices], test_quasars['g-slope'][sort_indices], color='blue', label = 'True g slope')\n",
    "plt.plot(test_quasars['zspec'][sort_indices], obs_slopes_g_20[sort_indices], color='black', label = 'Observed g slope@20 obs', alpha=0.7)\n",
    "plt.plot(test_quasars['zspec'][sort_indices], obs_slopes_g_3[sort_indices],color='cyan', alpha=0.5, label = 'Observed g slope@3 obs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('g-band DCR slope')\n",
    "plt.xlabel('Redshift')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GTR: I have ignored everything past here.\n",
    "#I was more concerned about making sure that we could reproduce the above plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate redshift PDFs for observed quasars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_PDFs(parameters, zshifts, feature_zshift_fit, feature_covariance):\n",
    "    \n",
    "    num_features = int((np.shape(parameters)[0]-1)/2)\n",
    "    num_of_quasars = np.shape(parameters)[1]\n",
    "    \n",
    "    #empty arrays to be filled\n",
    "    feature_distance =  np.zeros((num_of_quasars, num_features, len(zshifts)))\n",
    "    prob = np.zeros((num_of_quasars, len(zshifts)))\n",
    "    chi_squared =  np.zeros((num_of_quasars, len(zshifts)))\n",
    "    for i in range(num_of_quasars):\n",
    "        #empty arrays to be filled\n",
    "        features = np.zeros((num_features))\n",
    "        covariance_matrix_of_features = np.zeros((num_features,num_features))\n",
    "        \n",
    "        # loop through all the features (e.g. 'u-g', 'g-r', 'r-i', 'i-z', 'u-slope', 'g-slope')\n",
    "        for j in range(num_features):\n",
    "            for k in range(num_features):\n",
    "                if (j == k):\n",
    "                    if j < 4:\n",
    "                        # covaraince between the colors, on the diagonal\n",
    "                        covariance_matrix_of_features[j,k] = parameters[j+num_features,i]**2.0 + parameters[j+num_features+1,i]**2.0\n",
    "                    else:\n",
    "                        # covaraince between the slopes, on the diagonal\n",
    "                        covariance_matrix_of_features[j,k] = parameters[j+num_features+1,i]**2.0\n",
    "                elif abs(j - k) == 1:\n",
    "                    if j > k:\n",
    "                        if j < 4:\n",
    "                            # covaraince between the colors, just off the diagonal\n",
    "                            covariance_matrix_of_features[j,k] = -1.0*parameters[j+num_features,i]**2.0\n",
    "                    if k > j:\n",
    "                        if k < 4:\n",
    "                            # covaraince between the slopes, just off the diagonal\n",
    "                            covariance_matrix_of_features[j,k] = -1.0*parameters[k+num_features,i]**2.0\n",
    "            # difference between the features of this quasar and the regression calculate for all the quasars\n",
    "            features[j] = parameters[j,i]\n",
    "            feature_distance[i,j,:] = np.abs(features[j] - feature_zshift_fit[j,:])\n",
    "        for z in range(len(zshifts)):\n",
    "            # linear algebra from Weinstein et al. 2004\n",
    "            A = np.matrix(feature_distance[i,:,z])\n",
    "            B = np.matrix(covariance_matrix_of_features[:,:])\n",
    "            C = np.matrix(feature_covariance[:,:,z])\n",
    "            chi_squared[i,z] = np.dot(np.dot(A, (B + C).I), A.T)\n",
    "            try:\n",
    "                prob[i,z] = (np.exp(-1.0*chi_squared[i,z]/2.0))/(4.0*(math.pi**2.0)*(np.linalg.det(B + C)**0.5))\n",
    "                #if np.isnan(prob[i,z]):\n",
    "                    #prob[i,z] = 1e-250\n",
    "                    #prob[i,z] = (np.finfo(np.float64).tiny)\n",
    "            except:\n",
    "                prob[i,z] = 0.0\n",
    "        # normalize the probabilities\n",
    "        sum_of_array = np.nansum(prob[i,:], axis=0, dtype=np.float64)\n",
    "        try:\n",
    "            prob[i,:] = prob[i,:]/sum_of_array\n",
    "        except:\n",
    "            prob[i,:] = 0.0*prob[i,:]\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obs_parameters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0af3ba0ad7b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#calculate the pdf of the redshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexperiment_to_run\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'colors'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mobs_photoz_PDFs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_PDFs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzshifts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_fit_dcr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_covariance_dcr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mobs_photoz_PDFs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_PDFs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzshifts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_covariance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'obs_parameters' is not defined"
     ]
    }
   ],
   "source": [
    "#calculate the pdf of the redshift\n",
    "if experiment_to_run != 'colors':\n",
    "    obs_photoz_PDFs = calculate_PDFs(obs_parameters, zshifts, color_fit_dcr, color_covariance_dcr)\n",
    "else:\n",
    "    obs_photoz_PDFs = calculate_PDFs(obs_parameters, zshifts, color_fit, color_covariance)\n",
    "'''\n",
    "#dcr of opsim alone pdf\n",
    "obs_photoz_PDFs_dcr1 = calculate_PDFs(obs_parameters_dcr1, zshifts, color_fit_dcr, color_covariance_dcr)\n",
    "#dcr of opsim+longer observation time\n",
    "obs_photoz_PDFs_dcr2 = calculate_PDFs(obs_parameters_dcr2, zshifts, color_fit_dcr, color_covariance_dcr)\n",
    "#dcr of opsim+twilight survey\n",
    "obs_photoz_PDFs_dcr3 = calculate_PDFs(obs_parameters_dcr3, zshifts, color_fit_dcr, color_covariance_dcr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the peaks of the redshift PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photozPDF_to_pointestimate(photoz_PDFs, zshifts):\n",
    "    prob_threshold = 1.0/len(photoz_PDFs[0,:]) #threshold is above if all the probability were equally distributed\n",
    "    num_of_quasars = len(photoz_PDFs[:,0])\n",
    "    photoz_peaks = np.zeros((num_of_quasars))\n",
    "    for i in range(num_of_quasars):\n",
    "        zpeaks = np.array([])\n",
    "        zprobs = np.array([])\n",
    "        # all the non-nan values\n",
    "        good_idxs = np.arange(len(photoz_PDFs[i,:]), dtype=np.int)[~np.isnan(photoz_PDFs[i,:])]\n",
    "        # all the non-nan values above the probability threshold\n",
    "        good_idxs_high = good_idxs[np.where(photoz_PDFs[i,:][~np.isnan(photoz_PDFs[i,:])] > prob_threshold)[0]]\n",
    "        above_prob_threshold = list(good_idxs_high)\n",
    "        # only find peaks if there is a value above the threshold\n",
    "        if len(above_prob_threshold[1:-1]) > 1:\n",
    "            # find all the contiguous bins above the probability threshold, these are the bumps in the PDF\n",
    "            ranges = sum((list(t) for t in zip(above_prob_threshold, above_prob_threshold[1:]) if t[0]+1 != t[1]), [])\n",
    "            # add the edges of the redshift range back on\n",
    "            iranges = above_prob_threshold[0:1] + ranges + above_prob_threshold[-1:]\n",
    "            # find the peak of each of the bumps\n",
    "            for peaks in range(int(len(iranges)/2)):\n",
    "                peak_zmin = iranges[int(peaks*2):int(peaks*2) + 2][0]\n",
    "                peak_zmax = iranges[int(peaks*2):int(peaks*2) + 2][1]\n",
    "                peak_maxprob = zshifts[peak_zmin:peak_zmax+1][np.argmax(photoz_PDFs[i,peak_zmin:peak_zmax+1])]\n",
    "                # only count the peak if it isn't the minimum or maximum redshift bin\n",
    "                # there can be weird edge effects in the PDFs, so we don't want those peaks\n",
    "                if (peak_maxprob != zshifts[0]) and (peak_maxprob != zshifts[-1]):\n",
    "                    zpeaks = np.append(zpeaks, peak_maxprob)\n",
    "                    # the probability of that peak is all the area under the bump\n",
    "                    zprobs = np.append(zprobs, np.sum(photoz_PDFs[i,peak_zmin:peak_zmax+1]))\n",
    "                else:\n",
    "                    zpeaks = np.append(zpeaks, peak_maxprob)\n",
    "                    zprobs = np.append(zprobs, 0.0)\n",
    "            photoz_peaks[i] = zpeaks[np.argmax(zprobs)]\n",
    "        else:\n",
    "            photoz_peaks[i] = np.nan\n",
    "    return photoz_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_photoz_peaks = photozPDF_to_pointestimate(obs_photoz_PDFs, zshifts)\n",
    "#obs_photoz_peaks_dcr1 = photozPDF_to_pointestimate(obs_photoz_PDFs_dcr1, zshifts)\n",
    "#obs_photoz_peaks_dcr2 = photozPDF_to_pointestimate(obs_photoz_PDFs_dcr2, zshifts)\n",
    "#obs_photoz_peaks_dcr3 = photozPDF_to_pointestimate(obs_photoz_PDFs_dcr3, zshifts)\n",
    "print(obs_photoz_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = save_file_name\n",
    "test_quasars_zspec = test_quasars['zspec']\n",
    "if experiment_to_run != 'colors':\n",
    "    np.savez(fileName, \n",
    "             airmasses=airmasses,\n",
    "             filters=filters,\n",
    "             deltaSlope_g=deltaSlope_g, \n",
    "             deltaSlope_u=deltaSlope_u, \n",
    "             z_phot=obs_photoz_peaks,\n",
    "             z_true=test_quasars_zspec,\n",
    "             redshift=zshifts)\n",
    "else:\n",
    "    np.savez(fileName,\n",
    "            z_phot = obs_photoz_peaks,\n",
    "            z_true = test_quasars_zspec,\n",
    "            redshift=zshifts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write out the simulated quasars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quasars_zspec = test_quasars['zspec']\n",
    "\n",
    "with open('simulatedquasars_photozPDFs.dat', \"w\") as file_name:\n",
    "    file_name.write(\"#zspec photozpeak photozPDF\")\n",
    "    file_name.write(\"\\n\")\n",
    "    for i in range(len(test_quasars_zspec)):\n",
    "        file_name.write(\"%0.4f %0.4f \" % (test_quasars_zspec[i], obs_photoz_peaks[i]))\n",
    "        for j in range(len(obs_photoz_PDFs[i,:])):\n",
    "            file_name.write(\"%0.4f \" % (obs_photoz_PDFs[i,j]))\n",
    "        file_name.write(\"\\n\")\n",
    "\n",
    "with open('simulatedquasars_obsparameters.dat', \"w\") as file_name:\n",
    "    file_name.write(\"#zspec u-g g-r r-i i-z u-slope g-slope uerr gerr rerr ierr zerr u-slopeerr g-slopeerr\")\n",
    "    file_name.write(\"\\n\")\n",
    "    for i in range(len(test_quasars_zspec)):\n",
    "        for j in range(len(obs_parameters[:,i])):\n",
    "            file_name.write(\"%0.4f \" % (obs_parameters[j,i]))\n",
    "        file_name.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTR: Have everything below read in data files in order to produce plots.  Let's just make single panels instead of 2x2.  We can build those if need be.\n",
    "\n",
    "GTR: Add z_spec vs. zphot plots and Delta z histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the redshift quality metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photo_z_robust_stdev(z_est, z_true, zshifts):\n",
    "    \"\"\"\n",
    "    Sort the delta_z data into redshift bins in z_true.\n",
    "    Delta_z is defined as (z_true - z_est) / (1. + z_true).\n",
    "    \n",
    "    Calculate the robust standard deviation in each bin as a function of true redshift.\n",
    "    Robust standard deviation is defined as the standard deviation of delta_z in the bin where delta_z\n",
    "    is defined as (z_true - z_est) / (1. + z_true) and we trim the highest and lowest 25% of delta_z values.\n",
    "    \"\"\"\n",
    "\n",
    "    delta_z = (z_true - z_est) / (1. + z_true)\n",
    "    idx_sort = z_true.argsort()\n",
    "    delta_z_sort = delta_z[idx_sort]\n",
    "    z_true_sort = z_true[idx_sort]\n",
    "    idx_bins = z_true_sort.searchsorted(zshifts)\n",
    "    delta_z_binned = [delta_z_sort[idx_bins[i]:idx_bins[i+1]] for i in range(len(zshifts)-1)]\n",
    "    stdev_iqr_results = []\n",
    "    for delta_z_data in delta_z_binned:\n",
    "        if len(delta_z_data) == 0:\n",
    "            stdev_iqr_results.append(np.nan)\n",
    "            continue\n",
    "        bin_25 = np.percentile(delta_z_data, 25.)\n",
    "        bin_75 = np.percentile(delta_z_data, 75.)\n",
    "        diff = bin_75 - bin_25\n",
    "        stdev_iqr_results.append(diff/1.349)\n",
    "    return np.array(stdev_iqr_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the name of the file you want to plot from here\n",
    "#file_to_load = 'this_is_a_placeholder.npz'  #Defaults to file that was just created, but can be changed\n",
    "file_to_load = save_file_name\n",
    "#file_to_load = \"AstroMetric_TwilightDCR_[]_2obs.npz\"\n",
    "plot_data = np.load(file_to_load)\n",
    "print(file_to_load[:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate standard deviation of zphot over the interquartile range\n",
    "stdev_iqr = photo_z_robust_stdev(plot_data['z_phot'], plot_data['z_true'], plot_data['redshift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.xlabel('True Redshift')\n",
    "plt.ylabel('Standarad Deviation within Interquartile Range')\n",
    "plt.xlim(0.3,4)\n",
    "plt.ylim(0,0.4)\n",
    "plt.scatter(plot_data['redshift'][:-1], stdev_iqr)\n",
    "plot_save_name = file_to_load[:-4] + '_stdev_iqr_plot.pdf'\n",
    "plt.savefig(plot_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "plt.xlabel('True Redshift')\n",
    "plt.ylabel('Zphot')\n",
    "plt.scatter(plot_data['z_true'], plot_data['z_phot'])\n",
    "plot_save_name = file_to_load[:-4] + '_ztrue_vs_zphot_plot.pdf'\n",
    "plt.savefig(plot_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaZ = np.subtract(plot_data['z_true'], plot_data['z_phot'])\n",
    "data, bin_edges = np.histogram(deltaZ, bins='fd')\n",
    "bins = 0.5*(bin_edges[:-1]+bin_edges[1:])\n",
    "#z_err = np.divide(deltaZ, [1+z for z in plot_data['z_true']])\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.xlabel('deltaZ')\n",
    "plt.ylabel('Counts')\n",
    "plt.step(bins,data)\n",
    "plot_save_name = file_to_load[:-4] + '_deltaZ_hist_plot.pdf'\n",
    "plt.savefig(plot_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.hist(plot_data['deltaSlope_u'], bins=75, range=(-0.3,0.3))\n",
    "plt.title('Delta Slope u-band '+str(len(plot_data['airmasses'])))\n",
    "plt.subplot(122)\n",
    "plt.hist(plot_data['deltaSlope_g'], bins=75, range=(-0.3,0.3))\n",
    "plt.title('Delta Slope g-band '+str(len(plot_data['airmasses'])))\n",
    "filename = \"DeltaSlopeimgFiles/deltaSlopeHist\" + str(len(plot_data['airmasses']))\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
